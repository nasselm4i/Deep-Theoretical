{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nasselm4i/Deep-Theoretical/blob/main/SSL_Methods_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm38TXmirv_g"
      },
      "source": [
        "# Requirements\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fKKIQVO0rUuO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/htdt/self-supervised.git && pip install lightly\n",
        "%cd self-supervised\n",
        "!pip install wandb --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VW-SYD5ZsJvi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "from lightly.data import LightlyDataset\n",
        "from lightly.data.multi_view_collate import MultiViewCollate\n",
        "from lightly.loss import BarlowTwinsLoss\n",
        "from lightly.models.modules import BarlowTwinsProjectionHead\n",
        "from lightly.transforms.simclr_transform import SimCLRTransform\n",
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import wandb\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "import copy\n",
        "\n",
        "from lightly.loss import NegativeCosineSimilarity\n",
        "from lightly.models.modules import BYOLPredictionHead, BYOLProjectionHead\n",
        "from lightly.models.utils import deactivate_requires_grad, update_momentum\n",
        "from lightly.utils.scheduler import cosine_schedule\n",
        "\n",
        "####################################################################################\n",
        "\n",
        "from lightly.models.modules import SimSiamPredictionHead, SimSiamProjectionHead\n",
        "from lightly.transforms import SimSiamTransform\n",
        "\n",
        "\n",
        "from lightly.loss import VICRegLoss\n",
        "\n",
        "from lightly.loss.vicreg_loss import VICRegLoss\n",
        "from lightly.transforms.vicreg_transform import VICRegTransform\n",
        "\n",
        "from lightly.transforms.simclr_transform import SimCLRTransform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AztAO4TRsWNl"
      },
      "source": [
        "# Metrics for tracking Collapse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y31qg2rCsamv"
      },
      "source": [
        "## 1.1 Singular Value "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XzVQQblXsV9I"
      },
      "outputs": [],
      "source": [
        "def compute_singular_values(z0, z1):\n",
        "    \"\"\"\n",
        "    Compute the singular values of the cross-correlation matrix between the embeddings z0 and z1.\n",
        "    \n",
        "    Args:\n",
        "    - z0 (torch.Tensor): The embedding tensor for the first sequence, with shape (seq_len, embedding_dim).\n",
        "    - z1 (torch.Tensor): The embedding tensor for the second sequence, with shape (seq_len, embedding_dim).\n",
        "    \n",
        "    Returns:\n",
        "    - singular_values (np.ndarray): The singular values of the cross-correlation matrix, sorted in descending order.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "      # Compute cross-correlation matrix\n",
        "      c = cross_correlation_matrix(z0, z1)\n",
        "\n",
        "      # Compute singular values\n",
        "      svd = torch.svd(c)\n",
        "      singular_values = svd.S.cpu().detach().numpy()\n",
        "      return singular_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFnYbasgRU5u"
      },
      "source": [
        "## 1.2 Mean off diagonal Cross-Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ViSs2dtVRU5v"
      },
      "outputs": [],
      "source": [
        "def cross_correlation_matrix(z0, z1):\n",
        "  \"\"\"\n",
        "    Compute the cross-correlation matrix between the embeddings z0 and z1.\n",
        "    \n",
        "    Args:\n",
        "    - z0 (torch.Tensor): The embedding tensor for the first sequence, with shape (seq_len, embedding_dim).\n",
        "    - z1 (torch.Tensor): The embedding tensor for the second sequence, with shape (seq_len, embedding_dim).\n",
        "    \n",
        "    Returns:\n",
        "    - c (np.ndarray): the cross-correlation matrix.\n",
        "    \"\"\"\n",
        "  with torch.no_grad():\n",
        "      z0_centered = z0 - z0.mean(dim=0) / z0.std(0)\n",
        "      z1_centered = z1 - z1.mean(dim=0) / z1.std(0)\n",
        "      c = torch.mm(z0_centered.T, z1_centered) / (z0_centered.shape[0])\n",
        "      # std0 = z0_centered.std(dim=0, unbiased=False)\n",
        "      # std1 = z1_centered.std(dim=0, unbiased=False)\n",
        "      # c = c / torch.outer(std0, std1)\n",
        "      return c\n",
        "\n",
        "def cross_covariance_matrix(z0, z1):\n",
        "  pass\n",
        "\n",
        "def compute_average_off_correlation_matrix(z0,z1):\n",
        "  \"\"\"\n",
        "    Compute the average of the off diagonal of the cross-correlation matrix between the embeddings z0 and z1.\n",
        "    \n",
        "    Args:\n",
        "    - z0 (torch.Tensor): The embedding tensor for the first sequence, with shape (seq_len, embedding_dim).\n",
        "    - z1 (torch.Tensor): The embedding tensor for the second sequence, with shape (seq_len, embedding_dim).\n",
        "    \n",
        "    Returns:\n",
        "    - corr (float): The average of the off diagonal of the cross-correlation matrix.\n",
        "    \"\"\"\n",
        "  with torch.no_grad():\n",
        "    c = cross_correlation_matrix(z0, z1)\n",
        "    corr = (c.flatten()[c.shape[0]::c.shape[0]+1].mean().item())\n",
        "    return corr"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J_QhV_e8izU2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvRStrgyrXeT"
      },
      "source": [
        "# Methods"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "\n",
        "from lightly.data import LightlyDataset\n",
        "from lightly.data.multi_view_collate import MultiViewCollate\n",
        "from lightly.loss import BarlowTwinsLoss\n",
        "from lightly.models.modules import BarlowTwinsProjectionHead\n",
        "from lightly.transforms.simclr_transform import SimCLRTransform\n",
        "\n",
        "\n",
        "class BarlowTwins(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = BarlowTwinsProjectionHead(512, 2048, 2048)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(x)\n",
        "        return z\n",
        "\n",
        "\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = BarlowTwins(backbone)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "cifar10 = torchvision.datasets.CIFAR10(\"datasets/cifar10\", download=True)\n",
        "transform = SimCLRTransform(input_size=32)\n",
        "dataset = LightlyDataset.from_torch_dataset(cifar10, transform=transform)\n",
        "# or create a dataset from a folder containing images or videos:\n",
        "# dataset = LightlyDataset(\"path/to/folder\")\n",
        "\n",
        "collate_fn = MultiViewCollate()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=256,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "criterion = BarlowTwinsLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.06)\n",
        "\n",
        "print(\"Starting Training\")\n",
        "for epoch in range(10):\n",
        "    total_loss = 0\n",
        "    for (x0, x1), _, _ in dataloader:\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "        z0 = model(x0)\n",
        "        z1 = model(x1)\n",
        "        loss = criterion(z0, z1)\n",
        "        total_loss += loss.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}\")"
      ],
      "metadata": {
        "id": "jn2zfzGTi0yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhPRbzATrbkD"
      },
      "source": [
        "## 2.1 BarlowTwin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9tuBCxPrmcg"
      },
      "outputs": [],
      "source": [
        "class BarlowTwins(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = BarlowTwinsProjectionHead(512, 2048, 2048)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(x)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YFWcTy_tdUR"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"SSL-Methods\",\n",
        "    name=\"BarlowTwins-Vanilla-VICTransform\",\n",
        "    config={\n",
        "        \"max_epochs\": 10,\n",
        "        \"batch_size\": 256,\n",
        "        \"lr\": 0.06\n",
        "    })\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = BarlowTwins(backbone)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "cifar10 = torchvision.datasets.CIFAR10(\"datasets/cifar10\", download=True)\n",
        "# transform = SimCLRTransform(input_size=32)\n",
        "transform = VICRegTransform(input_size=32)\n",
        "dataset = LightlyDataset.from_torch_dataset(cifar10, transform=transform)\n",
        "\n",
        "collate_fn = MultiViewCollate()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "criterion = BarlowTwinsLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
        "\n",
        "print(\"Starting Training\")\n",
        "start = time.time()\n",
        "log_dict_BarlowTwins = {\"avg_loss\": [], \"avg_corr\": [], \"entropy_z0\": [], \"entropy_z1\": []}\n",
        "for epoch in tqdm(range(config.max_epochs)):\n",
        "    total_loss = 0\n",
        "    corr = 0\n",
        "    corr_count = 0\n",
        "    for (x0, x1), _, _ in dataloader:\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "        z0 = model(x0)\n",
        "        z1 = model(x1)\n",
        "        loss = criterion(z0, z1)\n",
        "        total_loss += loss.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_corr = compute_average_off_correlation_matrix(z0,z1)\n",
        "    entropy_z0 = entropy(z0)\n",
        "    entropy_z1 = entropy(z1)\n",
        "    log_dict_BarlowTwins[\"avg_loss\"].append(avg_loss)\n",
        "    log_dict_BarlowTwins[\"avg_corr\"].append(avg_corr)\n",
        "    log_dict_BarlowTwins[\"entropy_z0\"].append(entropy_z0)\n",
        "    log_dict_BarlowTwins[\"entropy_z1\"].append(entropy_z1)\n",
        "    # print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}, avg cross-correlation: {avg_corr}, mutual information: {mut_info} \")\n",
        "    wandb.log({\"avg_loss\": avg_loss, \"avg_corr\": avg_corr, \"entropy_z0\": entropy_z0, \"entropy_z1\": entropy_z1})\n",
        "\n",
        "# time to train \n",
        "end = time.time()\n",
        "train_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
        "print(\"Time for the training :\", train_time)\n",
        "# compute singular values\n",
        "singular_values_BarlowTwins = compute_singular_values(z0, z1)\n",
        "\n",
        "# plot singular values\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(singular_values_BarlowTwins.size),singular_values_BarlowTwins, label=f'Singular Values')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('Singular Value Index')\n",
        "ax.set_ylabel('Singular Value')\n",
        "wandb.log({\"Log Singular Values \": fig})\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'BarlowTwin-Vanilla.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G667-8cZrbhU"
      },
      "source": [
        "## 2.2 VICReg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "g-0GpVi-rpx1"
      },
      "outputs": [],
      "source": [
        "class VICReg(nn.Module): # Same as Barlow Twin\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = BarlowTwinsProjectionHead(512, 2048, 2048)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(x)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIwXa6Tqt-gR"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"SSL-Methods\",\n",
        "    name=\"VICReg-Vanilla\",\n",
        "    config={\n",
        "        \"max_epochs\": 40,\n",
        "        \"batch_size\": 256,\n",
        "        \"lr\": 0.06\n",
        "    })\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = VICReg(backbone)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "cifar10 = torchvision.datasets.CIFAR10(\"datasets/cifar10\", download=True)\n",
        "transform = VICRegTransform(input_size=32)\n",
        "dataset = LightlyDataset.from_torch_dataset(cifar10, transform=transform)\n",
        "\n",
        "collate_fn = MultiViewCollate()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "criterion = VICRegLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
        "\n",
        "print(\"Starting Training\")\n",
        "start = time.time()\n",
        "log_dict_VICReg = {\"avg_loss\": [], \"avg_corr\": [], \"entropy_z0\": [], \"entropy_z1\": []}\n",
        "for epoch in tqdm(range(config.max_epochs)):\n",
        "    total_loss = 0\n",
        "    for (x0, x1), _, _ in dataloader:\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "        z0 = model(x0)\n",
        "        z1 = model(x1)\n",
        "        loss = criterion(z0, z1)\n",
        "        total_loss += loss.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_corr = compute_average_off_correlation_matrix(z0,z1)\n",
        "    entropy_z0 = entropy(z0)\n",
        "    entropy_z1 = entropy(z1)\n",
        "    log_dict_VICReg[\"avg_loss\"].append(avg_loss)\n",
        "    log_dict_VICReg[\"avg_corr\"].append(avg_corr)\n",
        "    log_dict_VICReg[\"entropy_z0\"].append(entropy_z0)\n",
        "    log_dict_VICReg[\"entropy_z1\"].append(entropy_z1)\n",
        "    # print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}, avg cross-correlation: {avg_corr}, mutual information: {mut_info} \")\n",
        "    wandb.log({\"avg_loss\": avg_loss, \"avg_corr\": avg_corr, \"entropy_z0\": entropy_z0, \"entropy_z1\": entropy_z1})\n",
        "\n",
        "# time to train \n",
        "end = time.time()\n",
        "train_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
        "print(\"Time for the training :\", train_time)\n",
        "# compute singular values\n",
        "singular_values_VICReg = compute_singular_values(z0, z1)\n",
        "\n",
        "# plot singular values\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.plot(range(singular_values_VICReg.size),singular_values_VICReg, label=f'Singular Values')\n",
        "# ax.set_yscale('log')\n",
        "# ax.set_xlabel('Singular Value Index')\n",
        "# ax.set_ylabel('Singular Value')\n",
        "# wandb.log({\"Log Singular Values \": fig})\n",
        "data = [[x, y] for (x, y) in zip(range(singular_values_VICReg.size), singular_values_VICReg)]\n",
        "table = wandb.Table(data=data, columns = [\"Singular Value Index\", \"Log of Singular Values\"])\n",
        "wandb.log(\n",
        "    {\"my_custom_plot_id\" : wandb.plot.line(table, \"x\", \"y\",\n",
        "           title=\"Log of Singular Values for Tracking Dimensional Collapse\")})\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'VICReg-Vanilla.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4wLufQarbNc"
      },
      "source": [
        "## 2.4 SimSiam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cOniOUxrXJq"
      },
      "outputs": [],
      "source": [
        "class SimSiam(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = SimSiamProjectionHead(512, 512, 128)\n",
        "        self.prediction_head = SimSiamPredictionHead(128, 64, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(f)\n",
        "        p = self.prediction_head(z)\n",
        "        z = z.detach()\n",
        "        return z, p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tR0kEEM8vCEF"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIYwWkzivD9U"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"SSL-Methods\",\n",
        "    name=\"SimSiam-Vanilla\",\n",
        "    config={\n",
        "        \"max_epochs\": 10,\n",
        "        \"batch_size\": 256,\n",
        "        \"lr\": 0.06\n",
        "    })\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = SimSiam(backbone)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "cifar10 = torchvision.datasets.CIFAR10(\"datasets/cifar10\", download=True)\n",
        "transform = SimSiamTransform(input_size=32)\n",
        "dataset = LightlyDataset.from_torch_dataset(cifar10, transform=transform)\n",
        "\n",
        "collate_fn = MultiViewCollate()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "criterion = NegativeCosineSimilarity()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
        "\n",
        "print(\"Starting Training\")\n",
        "start = time.time()\n",
        "log_dict_SimSiam = {\"avg_loss\": [], \"avg_corr\": [], \"entropy_z0\": [], \"entropy_z1\": []}\n",
        "for epoch in tqdm(range(config.max_epochs)):\n",
        "    total_loss = 0\n",
        "    for (x0, x1), _, _ in dataloader:\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "        z0, p0 = model(x0)\n",
        "        z1, p1 = model(x1)\n",
        "        loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
        "        total_loss += loss.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_corr = compute_average_off_correlation_matrix(z0,z1)\n",
        "    entropy_z0 = entropy(z0)\n",
        "    entropy_z1 = entropy(z1)\n",
        "    log_dict_SimSiam[\"avg_loss\"].append(avg_loss)\n",
        "    log_dict_SimSiam[\"avg_corr\"].append(avg_corr)\n",
        "    log_dict_SimSiam[\"entropy_z0\"].append(entropy_z0)\n",
        "    log_dict_SimSiam[\"entropy_z1\"].append(entropy_z1)\n",
        "    # print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}, avg cross-correlation: {avg_corr}, mutual information: {mut_info} \")\n",
        "    wandb.log({\"avg_loss\": avg_loss, \"avg_corr\": avg_corr, \"entropy_z0\": entropy_z0, \"entropy_z1\": entropy_z1})\n",
        "\n",
        "# time to train \n",
        "end = time.time()\n",
        "train_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
        "print(\"Time for the training :\", train_time)\n",
        "# compute singular values\n",
        "singular_values_SimSiam = compute_singular_values(z0, z1)\n",
        "\n",
        "# plot singular values\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(singular_values_SimSiam.size),singular_values_SimSiam, label=f'Singular Values')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('Singular Value Index')\n",
        "ax.set_ylabel('Singular Value')\n",
        "wandb.log({\"Log Singular Values \": fig})\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'SimSiam-Vanilla.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugRsMgQor_eQ"
      },
      "source": [
        "## 2.5 BYOL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9vkZv59uGIp"
      },
      "outputs": [],
      "source": [
        "class BYOL(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = BYOLProjectionHead(512, 1024, 256)\n",
        "        self.prediction_head = BYOLPredictionHead(256, 1024, 256)\n",
        "\n",
        "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
        "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
        "\n",
        "        deactivate_requires_grad(self.backbone_momentum)\n",
        "        deactivate_requires_grad(self.projection_head_momentum)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(y)\n",
        "        p = self.prediction_head(z)\n",
        "        return p\n",
        "\n",
        "    def forward_momentum(self, x):\n",
        "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
        "        z = self.projection_head_momentum(y)\n",
        "        z = z.detach()\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNwgELXhsD-3"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"SSL-Methods\",\n",
        "    name=\"BYOL-Vanilla\",\n",
        "    config={\n",
        "        \"max_epochs\": 10,\n",
        "        \"batch_size\": 256,\n",
        "        \"lr\": 0.06\n",
        "    })\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = BYOL(backbone)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "cifar10 = torchvision.datasets.CIFAR10(\"datasets/cifar10\", download=True)\n",
        "transform = SimCLRTransform(input_size=32)\n",
        "dataset = LightlyDataset.from_torch_dataset(cifar10, transform=transform)\n",
        "# or create a dataset from a folder containing images or videos:\n",
        "# dataset = LightlyDataset(\"path/to/folder\")\n",
        "\n",
        "collate_fn = MultiViewCollate()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "criterion = NegativeCosineSimilarity()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
        "\n",
        "\n",
        "print(\"Starting Training\")\n",
        "start = time.time()\n",
        "log_dict_BYOL = {\"avg_loss\": [], \"avg_corr\": [], \"entropy_z0\": [], \"entropy_z1\": []}\n",
        "for epoch in tqdm(range(config.max_epochs)):\n",
        "    total_loss = 0\n",
        "    momentum_val = cosine_schedule(epoch, config.max_epochs, 0.996, 1)\n",
        "    for (x0, x1), _, _ in dataloader:\n",
        "        update_momentum(model.backbone, model.backbone_momentum, m=momentum_val)\n",
        "        update_momentum(\n",
        "            model.projection_head, model.projection_head_momentum, m=momentum_val\n",
        "        )\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "        p0 = model(x0)\n",
        "        z0 = model.forward_momentum(x0)\n",
        "        p1 = model(x1)\n",
        "        z1 = model.forward_momentum(x1)\n",
        "        loss = 0.5 * (criterion(p0, z1) + criterion(p1, z0))\n",
        "        total_loss += loss.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_corr = compute_average_off_correlation_matrix(z0,z1)\n",
        "    entropy_z0 = entropy(z0)\n",
        "    entropy_z1 = entropy(z1)\n",
        "    log_dict_BYOL[\"avg_loss\"].append(avg_loss)\n",
        "    log_dict_BYOL[\"avg_corr\"].append(avg_corr)\n",
        "    log_dict_BYOL[\"entropy_z0\"].append(entropy_z0)\n",
        "    log_dict_BYOL[\"entropy_z1\"].append(entropy_z1)\n",
        "    # print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}, avg cross-correlation: {avg_corr}, mutual information: {mut_info} \")\n",
        "    wandb.log({\"avg_loss\": avg_loss, \"avg_corr\": avg_corr, \"entropy_z0\": entropy_z0, \"entropy_z1\": entropy_z1})\n",
        "\n",
        "# time to train \n",
        "end = time.time()\n",
        "train_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
        "print(\"Time for the training :\", train_time)\n",
        "# compute singular values\n",
        "singular_values_BYOL = compute_singular_values(z0, z1)\n",
        "\n",
        "# plot singular values\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(singular_values_BYOL.size),singular_values_BYOL, label=f'Singular Values')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('Singular Value Index')\n",
        "ax.set_ylabel('Singular Value')\n",
        "wandb.log({\"Log Singular Values \": fig})\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'BYOL-Vanilla.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ooCAeRUo2Eli"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aow6wQ5X2Elj"
      },
      "source": [
        "## 3.1 BarlowTwin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egO1veV5jJ-M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "wandb.init(\n",
        "    project=\"BarlowTwin-Vanilla-Test\",\n",
        "    config={\n",
        "        \"max_epochs\": 800,\n",
        "        \"batch_size\": 256,\n",
        "        \"lr\": 0.2\n",
        "    })\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "# Load the trained BarlowTwin model\n",
        "state_dict = torch.load('BarlowTwin-Vanilla.pth')\n",
        "\n",
        "# Create a new model and load the state_dict\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "backbone = BarlowTwins(backbone)\n",
        "backbone.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "head = nn.Linear(2048, 10)\n",
        "head.weight.data.normal_(mean=0.0, std=0.01)\n",
        "head.bias.data.zero_()\n",
        "model = nn.Sequential(backbone, head)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "backbone.requires_grad_(False)\n",
        "head.requires_grad_(True)\n",
        "\n",
        "model.train()\n",
        "\n",
        "# Load the CIFAR10 train set\n",
        "cifar10 = torchvision.datasets.CIFAR10(\"datasets/cifar10\", download=True)\n",
        "dataset = LightlyDataset.from_torch_dataset(cifar10, transform=transforms.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "# Train the linear classifier\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(head.parameters(), lr=config.lr)\n",
        "\n",
        "print(\"Start Training Linear Classifier\")\n",
        "start = time.time()\n",
        "log_dict_BarlowTwins_LC = {\"Loss\": []}\n",
        "for epoch in range(config.max_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels, _) in enumerate(train_loader, 0):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        running_loss += loss.item()\n",
        "    log_dict_BarlowTwins_LC[\"Loss\"].append(running_loss/(i+1))\n",
        "    wandb.log({\"Loss\": running_loss / (i+1)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kq5vZEu2craK"
      },
      "outputs": [],
      "source": [
        "# Load the CIFAR10 test set\n",
        "cifar10_test = torchvision.datasets.CIFAR10(\"datasets/cifar10\", train=False, download=True)\n",
        "dataset_test = LightlyDataset.from_torch_dataset(cifar10_test, transform=transforms.ToTensor())\n",
        "dataloader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=config.batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for inputs, targets, _ in dataloader_test:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Compute the model's predictions\n",
        "        outputs = model(inputs)\n",
        "        print(outputs)\n",
        "        # print(targets)\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, targets)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute the number of correctly classified samples\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        total_correct += (predictions == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "    # Compute the accuracy and average loss\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_loss = total_loss / len(dataloader_test)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Average Test Loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A30xLbF62Elk"
      },
      "source": [
        "## 3.2 VICReg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0DTd0nO2Elk"
      },
      "outputs": [],
      "source": [
        "model_path = 'VICReg-Vanilla.pth'\n",
        "\n",
        "# Define the linear classifier\n",
        "linear_classifier = torch.nn.Linear(2048, 10)\n",
        "\n",
        "# Load the trained BarlowTwin model\n",
        "barlowtwins_model = torch.load(model_path)\n",
        "\n",
        "# Freeze the BarlowTwin layers\n",
        "for param in barlowtwins_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last layer with the linear classifier\n",
        "barlowtwins_model.fc = linear_classifier\n",
        "\n",
        "# Load the CIFAR10 test set\n",
        "test_dataset = dataset.CIFAR10(cifar10, train=False, download=True, transform=transforms.ToTensor())\n",
        "test_loader = dataloader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(linear_classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "barlowtwins_model.eval()\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for inputs, targets in test_loader:\n",
        "        # Move the inputs and targets to the device (GPU or CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Compute the model's predictions\n",
        "        outputs = barlowtwins_model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, targets)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute the number of correctly classified samples\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        total_correct += (predictions == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "    # Compute the accuracy and average loss\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Average Test Loss: {avg_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0PiRWIG2Ell"
      },
      "source": [
        "## 3.4 SimSiam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqnLJYL_2Ell"
      },
      "outputs": [],
      "source": [
        "model_path = 'BarlowTwinV0.pth'\n",
        "\n",
        "# Define the linear classifier\n",
        "linear_classifier = torch.nn.Linear(2048, 10)\n",
        "\n",
        "# Load the trained BarlowTwin model\n",
        "barlowtwins_model = torch.load(model_path)\n",
        "\n",
        "# Freeze the BarlowTwin layers\n",
        "for param in barlowtwins_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last layer with the linear classifier\n",
        "barlowtwins_model.fc = linear_classifier\n",
        "\n",
        "# Load the CIFAR10 test set\n",
        "test_dataset = dataset.CIFAR10(cifar10, train=False, download=True, transform=transforms.ToTensor())\n",
        "test_loader = dataloader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(linear_classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "barlowtwins_model.eval()\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for inputs, targets in test_loader:\n",
        "        # Move the inputs and targets to the device (GPU or CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Compute the model's predictions\n",
        "        outputs = barlowtwins_model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, targets)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute the number of correctly classified samples\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        total_correct += (predictions == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "    # Compute the accuracy and average loss\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Average Test Loss: {avg_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jSl0hzw2Ell"
      },
      "source": [
        "## 3.5 BYOL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr1J2goT2Ell"
      },
      "outputs": [],
      "source": [
        "model_path = 'BarlowTwinV0.pth'\n",
        "\n",
        "# Define the linear classifier\n",
        "linear_classifier = torch.nn.Linear(2048, 10)\n",
        "\n",
        "# Load the trained BarlowTwin model\n",
        "barlowtwins_model = torch.load(model_path)\n",
        "\n",
        "# Freeze the BarlowTwin layers\n",
        "for param in barlowtwins_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last layer with the linear classifier\n",
        "barlowtwins_model.fc = linear_classifier\n",
        "\n",
        "# Load the CIFAR10 test set\n",
        "test_dataset = dataset.CIFAR10(cifar10, train=False, download=True, transform=transforms.ToTensor())\n",
        "test_loader = dataloader(test_dataset, batch_size=256, shuffle=False)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(linear_classifier.parameters(), lr=0.001)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "barlowtwins_model.eval()\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for inputs, targets in test_loader:\n",
        "        # Move the inputs and targets to the device (GPU or CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Compute the model's predictions\n",
        "        outputs = barlowtwins_model(inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs, targets)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Compute the number of correctly classified samples\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        total_correct += (predictions == targets).sum().item()\n",
        "        total_samples += targets.size(0)\n",
        "\n",
        "    # Compute the accuracy and average loss\n",
        "    accuracy = total_correct / total_samples\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n",
        "    print(f'Average Test Loss: {avg_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcEH5MC-H_k9"
      },
      "source": [
        "# Methods - Change 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op2q_SESH_k-"
      },
      "source": [
        "## 2.1 BarlowTwin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "KYrR3JThW3Zd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "\n",
        "class BarlowTwinsLossChanged(torch.nn.Module):\n",
        "    def __init__(self, lambda_param: float = 5e-3, gather_distributed: bool = False, normalized:bool = True, redundancy:bool = True ):\n",
        "        \"\"\"Lambda param configuration with default value like in [0]\n",
        "\n",
        "        Args:\n",
        "            lambda_param:\n",
        "                Parameter for importance of redundancy reduction term.\n",
        "                Defaults to 5e-3 [0].\n",
        "            gather_distributed:\n",
        "                If True then the cross-correlation matrices from all gpus are\n",
        "                gathered and summed before the loss calculation.\n",
        "        \"\"\"\n",
        "        super(BarlowTwinsLossChanged, self).__init__()\n",
        "        self.lambda_param = lambda_param\n",
        "        self.gather_distributed = gather_distributed\n",
        "        self.normalized = normalized\n",
        "        self.redundancy = redundancy\n",
        "    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) -> torch.Tensor:\n",
        "        device = z_a.device\n",
        "        if self.normalized :\n",
        "          # normalize repr. along the batch dimension\n",
        "          z_a_norm = (z_a - z_a.mean(0)) / z_a.std(0)  # NxD\n",
        "          z_b_norm = (z_b - z_b.mean(0)) / z_b.std(0)  # NxD\n",
        "\n",
        "          N = z_a.size(0)\n",
        "          D = z_a.size(1)\n",
        "\n",
        "          # cross-correlation matrix\n",
        "          c = torch.mm(z_a_norm.T, z_b_norm) / N  # DxD\n",
        "        else :\n",
        "          c = torch.mm(z_a.T, z_b) / N  # DxD\n",
        "        # sum cross-correlation matrix between multiple gpus\n",
        "        if self.gather_distributed and dist.is_initialized():\n",
        "            world_size = dist.get_world_size()\n",
        "            if world_size > 1:\n",
        "                c = c / world_size\n",
        "                dist.all_reduce(c)\n",
        "\n",
        "        # loss\n",
        "        c_diff = (c - torch.eye(D, device=device)).pow(2)  # DxD\n",
        "        if self.redundancy:\n",
        "          # multiply off-diagonal elems of c_diff by lambda\n",
        "          c_diff[~torch.eye(D, dtype=bool)] *= self.lambda_param\n",
        "        loss = c_diff.sum()\n",
        "\n",
        "        return loss\n",
        "\n",
        "class BarlowTwins(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = BarlowTwinsProjectionHead(512, 2048, 2048)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(x)\n",
        "        return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHLSRxbGH_k-"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"SSL-Methods\",\n",
        "    name=\"BarlowTwins-WO-BN\",\n",
        "    notes=\"...\",\n",
        "    config={\n",
        "        \"max_epochs\": 20,\n",
        "        \"batch_size\": 256,\n",
        "        \"lr\": 0.06\n",
        "    })\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = BarlowTwins(backbone)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "cifar10 = torchvision.datasets.CIFAR10(\"datasets/cifar10\", download=True)\n",
        "transform = SimCLRTransform(input_size=32)\n",
        "dataset = LightlyDataset.from_torch_dataset(cifar10, transform=transform)\n",
        "\n",
        "collate_fn = MultiViewCollate()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "criterion = BarlowTwinsLossChanged(normalized=False)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
        "\n",
        "print(\"Starting Training\")\n",
        "start = time.time()\n",
        "log_dict_BarlowTwins = {\"avg_loss\": [], \"avg_corr\": [], \"entropy_z0\": [], \"entropy_z1\": []}\n",
        "for epoch in tqdm(range(config.max_epochs)):\n",
        "    total_loss = 0\n",
        "    corr = 0\n",
        "    corr_count = 0\n",
        "    for (x0, x1), _, _ in dataloader:\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "        z0 = model(x0)\n",
        "        z1 = model(x1)\n",
        "        loss = criterion(z0, z1)\n",
        "        total_loss += loss.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_corr = compute_average_off_correlation_matrix(z0,z1)\n",
        "    entropy_z0 = entropy(z0)\n",
        "    entropy_z1 = entropy(z1)\n",
        "    log_dict_BarlowTwins[\"avg_loss\"].append(avg_loss)\n",
        "    log_dict_BarlowTwins[\"avg_corr\"].append(avg_corr)\n",
        "    log_dict_BarlowTwins[\"entropy_z0\"].append(entropy_z0)\n",
        "    log_dict_BarlowTwins[\"entropy_z1\"].append(entropy_z1)\n",
        "    # print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}, avg cross-correlation: {avg_corr}, mutual information: {mut_info} \")\n",
        "    wandb.log({\"avg_loss\": avg_loss, \"avg_corr\": avg_corr, \"entropy_z0\": entropy_z0, \"entropy_z1\": entropy_z1})\n",
        "\n",
        "# time to train \n",
        "end = time.time()\n",
        "train_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
        "print(\"Time for the training :\", train_time)\n",
        "# compute singular values\n",
        "singular_values_BarlowTwins = compute_singular_values(z0, z1)\n",
        "\n",
        "# plot singular values\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(singular_values_BarlowTwins.size),singular_values_BarlowTwins, label=f'Singular Values')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('Singular Value Index')\n",
        "ax.set_ylabel('Singular Value')\n",
        "wandb.log({\"Log Singular Values \": fig})\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'BarlowTwin-Vanilla.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w86mRoIH_k_"
      },
      "source": [
        "## 2.2 VICReg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mwlnV4FuLQM7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "from lightly.utils.dist import gather\n",
        "\n",
        "\n",
        "class VICRegLossChanged(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        lambda_param: float = 25.0,\n",
        "        mu_param: float = 25.0,\n",
        "        nu_param: float = 1.0,\n",
        "        gather_distributed: bool = False,\n",
        "        eps=0.0001,\n",
        "        covariance: bool = False,\n",
        "        variance: bool = False,\n",
        "    ):\n",
        "        super(VICRegLossChanged, self).__init__()\n",
        "        self.lambda_param = lambda_param\n",
        "        self.mu_param = mu_param\n",
        "        self.nu_param = nu_param\n",
        "        self.gather_distributed = gather_distributed\n",
        "        self.eps = eps\n",
        "        self.covariance = covariance\n",
        "        self.variance = variance\n",
        "\n",
        "    def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Returns VICReg loss.\n",
        "\n",
        "        Args:\n",
        "            z_a:\n",
        "                Tensor with shape (batch_size, ..., dim).\n",
        "            z_b:\n",
        "                Tensor with shape (batch_size, ..., dim).\n",
        "        \"\"\"\n",
        "        assert (\n",
        "            z_a.shape[0] > 1 and z_b.shape[0] > 1\n",
        "        ), f\"z_a and z_b must have batch size > 1 but found {z_a.shape[0]} and {z_b.shape[0]}\"\n",
        "        assert (\n",
        "            z_a.shape == z_b.shape\n",
        "        ), f\"z_a and z_b must have same shape but found {z_a.shape} and {z_b.shape}.\"\n",
        "\n",
        "        # invariance term of the loss\n",
        "        inv_loss = invariance_loss(x=z_a, y=z_b)\n",
        "\n",
        "        # gather all batches\n",
        "        if self.gather_distributed and dist.is_initialized():\n",
        "            world_size = dist.get_world_size()\n",
        "            if world_size > 1:\n",
        "                z_a = torch.cat(gather(z_a), dim=0)\n",
        "                z_b = torch.cat(gather(z_b), dim=0)\n",
        "\n",
        "        var_loss = 0.5 * (variance_loss(x=z_a, eps=self.eps) + variance_loss(x=z_b, eps=self.eps))\n",
        "        cov_loss = covariance_loss(x=z_a) + covariance_loss(x=z_b)\n",
        "        \n",
        "        if self.variance:\n",
        "          loss = (\n",
        "            self.lambda_param * inv_loss\n",
        "            + self.nu_param * cov_loss\n",
        "        )\n",
        "          return loss\n",
        "        if self.covariance:\n",
        "          loss = (\n",
        "            self.lambda_param * inv_loss\n",
        "            + self.mu_param * var_loss\n",
        "        )\n",
        "          return loss\n",
        "        loss = (\n",
        "            self.lambda_param * inv_loss\n",
        "            + self.mu_param * var_loss\n",
        "            + self.nu_param * cov_loss\n",
        "        )\n",
        "        return loss\n",
        "\n",
        "\n",
        "def invariance_loss(x: Tensor, y: Tensor) -> Tensor:\n",
        "    \"\"\"Returns VICReg invariance loss.\n",
        "\n",
        "    Args:\n",
        "        x:\n",
        "            Tensor with shape (batch_size, ..., dim).\n",
        "        y:\n",
        "            Tensor with shape (batch_size, ..., dim).\n",
        "    \"\"\"\n",
        "    return F.mse_loss(x, y)\n",
        "\n",
        "\n",
        "def variance_loss(x: Tensor, eps: float = 0.0001) -> Tensor:\n",
        "    \"\"\"Returns VICReg variance loss.\n",
        "\n",
        "    Args:\n",
        "        x:\n",
        "            Tensor with shape (batch_size, ..., dim).\n",
        "        eps:\n",
        "            Epsilon for numerical stability.\n",
        "    \"\"\"\n",
        "    x = x - x.mean(dim=0)\n",
        "    std = torch.sqrt(x.var(dim=0) + eps)\n",
        "    loss = torch.mean(F.relu(1.0 - std))\n",
        "    return loss\n",
        "\n",
        "\n",
        "def covariance_loss(x: Tensor) -> Tensor:\n",
        "    \"\"\"Returns VICReg covariance loss.\n",
        "\n",
        "    Generalized version of the covariance loss with support for tensors with more than\n",
        "    two dimensions. Adapted from VICRegL:\n",
        "    https://github.com/facebookresearch/VICRegL/blob/803ae4c8cd1649a820f03afb4793763e95317620/main_vicregl.py#L299\n",
        "\n",
        "    Args:\n",
        "        x:\n",
        "            Tensor with shape (batch_size, ..., dim).\n",
        "    \"\"\"\n",
        "    x = x - x.mean(dim=0)\n",
        "    batch_size = x.size(0)\n",
        "    dim = x.size(-1)\n",
        "    # nondiag_mask has shape (dim, dim) with 1s on all non-diagonal entries.\n",
        "    nondiag_mask = ~torch.eye(dim, device=x.device, dtype=torch.bool)\n",
        "    # cov has shape (..., dim, dim)\n",
        "    cov = torch.einsum(\"b...c,b...d->...cd\", x, x) / (batch_size - 1)\n",
        "    loss = cov[..., nondiag_mask].pow(2).sum(-1) / dim\n",
        "    return loss.mean()\n",
        "\n",
        "class VICReg(nn.Module): # Same as Barlow Twin\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = BarlowTwinsProjectionHead(512, 2048, 2048)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(x)\n",
        "        return z\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRbg7jMpH_k_"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"SSL-Methods\",\n",
        "    name=\"VICReg-WO-Variance-20\",\n",
        "    notes=\"Without Variance\",\n",
        "    config={\n",
        "        \"max_epochs\": 20,\n",
        "        \"batch_size\": 256,\n",
        "        \"lr\": 0.06\n",
        "    })\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = VICReg(backbone)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "cifar10 = torchvision.datasets.CIFAR10(\"datasets/cifar10\", download=True)\n",
        "transform = VICRegTransform(input_size=32)\n",
        "dataset = LightlyDataset.from_torch_dataset(cifar10, transform=transform)\n",
        "\n",
        "collate_fn = MultiViewCollate()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "criterion = VICRegLossChanged(variance=True) # True = removed\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
        "\n",
        "print(\"Starting Training\")\n",
        "start = time.time()\n",
        "log_dict_VICReg = {\"avg_loss\": [], \"avg_corr\": [], \"entropy_z0\": [], \"entropy_z1\": []}\n",
        "for epoch in tqdm(range(config.max_epochs)):\n",
        "    total_loss = 0\n",
        "    for (x0, x1), _, _ in dataloader:\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "        z0 = model(x0)\n",
        "        z1 = model(x1)\n",
        "        loss = criterion(z0, z1)\n",
        "        total_loss += loss.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    \n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_corr = compute_average_off_correlation_matrix(z0,z1)\n",
        "    entropy_z0 = entropy(z0)\n",
        "    entropy_z1 = entropy(z1)\n",
        "    log_dict_VICReg[\"avg_loss\"].append(avg_loss)\n",
        "    log_dict_VICReg[\"avg_corr\"].append(avg_corr)\n",
        "    log_dict_VICReg[\"entropy_z0\"].append(entropy_z0)\n",
        "    log_dict_VICReg[\"entropy_z1\"].append(entropy_z1)\n",
        "    # print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}, avg cross-correlation: {avg_corr}, mutual information: {mut_info} \")\n",
        "    wandb.log({\"avg_loss\": avg_loss, \"avg_corr\": avg_corr, \"entropy_z0\": entropy_z0, \"entropy_z1\": entropy_z1})\n",
        "\n",
        "# time to train \n",
        "end = time.time()\n",
        "train_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
        "print(\"Time for the training :\", train_time)\n",
        "# compute singular values\n",
        "singular_values_VICReg = compute_singular_values(z0, z1)\n",
        "\n",
        "# plot singular values\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(singular_values_VICReg.size),singular_values_VICReg, label=f'Singular Values')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('Singular Value Index')\n",
        "ax.set_ylabel('Singular Value')\n",
        "wandb.log({\"Log Singular Values \": fig})\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'VICReg-Changed.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgQdv5j0H_lA"
      },
      "source": [
        "## 2.4 SimSiam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmhUN-alH_lA"
      },
      "outputs": [],
      "source": [
        "class SimSiam(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = SimSiamProjectionHead(512, 512, 128)\n",
        "        self.prediction_head = SimSiamPredictionHead(128, 64, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(f)\n",
        "        p = self.prediction_head(z)\n",
        "        z = z.detach()\n",
        "        return z, p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl-JNkqEH_lA"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIQUxMahH_lA"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"SSL-Methods\",\n",
        "    name=\"SimSiam-Change-1\",\n",
        "    notes=\"...\",\n",
        "    config={\n",
        "        \"max_epochs\": 10,\n",
        "        \"batch_size\": 256,\n",
        "        \"lr\": 0.06\n",
        "    })\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = SimSiam(backbone)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "cifar10 = torchvision.datasets.CIFAR10(\"datasets/cifar10\", download=True)\n",
        "transform = SimSiamTransform(input_size=32)\n",
        "dataset = LightlyDataset.from_torch_dataset(cifar10, transform=transform)\n",
        "\n",
        "collate_fn = MultiViewCollate()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "criterion = NegativeCosineSimilarity()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
        "\n",
        "print(\"Starting Training\")\n",
        "start = time.time()\n",
        "log_dict_SimSiam = {\"avg_loss\": [], \"avg_corr\": [], \"entropy_z0\": [], \"entropy_z1\": []}\n",
        "for epoch in tqdm(range(config.max_epochs)):\n",
        "    total_loss = 0\n",
        "    for (x0, x1), _, _ in dataloader:\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "        z0, p0 = model(x0)\n",
        "        z1, p1 = model(x1)\n",
        "        loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
        "        total_loss += loss.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_corr = compute_average_off_correlation_matrix(z0,z1)\n",
        "    entropy_z0 = entropy(z0)\n",
        "    entropy_z1 = entropy(z1)\n",
        "    log_dict_SimSiam[\"avg_loss\"].append(avg_loss)\n",
        "    log_dict_SimSiam[\"avg_corr\"].append(avg_corr)\n",
        "    log_dict_SimSiam[\"entropy_z0\"].append(entropy_z0)\n",
        "    log_dict_SimSiam[\"entropy_z1\"].append(entropy_z1)\n",
        "    # print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}, avg cross-correlation: {avg_corr}, mutual information: {mut_info} \")\n",
        "    wandb.log({\"avg_loss\": avg_loss, \"avg_corr\": avg_corr, \"entropy_z0\": entropy_z0, \"entropy_z1\": entropy_z1})\n",
        "\n",
        "# time to train \n",
        "end = time.time()\n",
        "train_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
        "print(\"Time for the training :\", train_time)\n",
        "# compute singular values\n",
        "singular_values_SimSiam = compute_singular_values(z0, z1)\n",
        "\n",
        "# plot singular values\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(singular_values_SimSiam.size),singular_values_SimSiam, label=f'Singular Values')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('Singular Value Index')\n",
        "ax.set_ylabel('Singular Value')\n",
        "wandb.log({\"Log Singular Values \": fig})\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'SimSiam-Vanilla.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBWRHtSjH_lA"
      },
      "source": [
        "## 2.5 BYOL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQvZ04HxH_lA"
      },
      "outputs": [],
      "source": [
        "class BYOL(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = backbone\n",
        "        self.projection_head = BYOLProjectionHead(512, 1024, 256)\n",
        "        self.prediction_head = BYOLPredictionHead(256, 1024, 256)\n",
        "\n",
        "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
        "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
        "\n",
        "        deactivate_requires_grad(self.backbone_momentum)\n",
        "        deactivate_requires_grad(self.projection_head_momentum)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.backbone(x).flatten(start_dim=1)\n",
        "        z = self.projection_head(y)\n",
        "        p = self.prediction_head(z)\n",
        "        return p\n",
        "\n",
        "    def forward_momentum(self, x):\n",
        "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
        "        z = self.projection_head_momentum(y)\n",
        "        z = z.detach()\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kajx-3avH_lB"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"SSL-Methods\",\n",
        "    name=\"BYOL-Change-1\",\n",
        "    notes=\"...\",\n",
        "    config={\n",
        "        \"max_epochs\": 10,\n",
        "        \"batch_size\": 256,\n",
        "        \"lr\": 0.06\n",
        "    })\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "resnet = torchvision.models.resnet18()\n",
        "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
        "model = BYOL(backbone)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "cifar10 = torchvision.datasets.CIFAR10(\"datasets/cifar10\", download=True)\n",
        "transform = SimCLRTransform(input_size=32)\n",
        "dataset = LightlyDataset.from_torch_dataset(cifar10, transform=transform)\n",
        "# or create a dataset from a folder containing images or videos:\n",
        "# dataset = LightlyDataset(\"path/to/folder\")\n",
        "\n",
        "collate_fn = MultiViewCollate()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=config.batch_size,\n",
        "    collate_fn=collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=8,\n",
        ")\n",
        "\n",
        "criterion = NegativeCosineSimilarity()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
        "\n",
        "\n",
        "print(\"Starting Training\")\n",
        "start = time.time()\n",
        "log_dict_BYOL = {\"avg_loss\": [], \"avg_corr\": [], \"entropy_z0\": [], \"entropy_z1\": []}\n",
        "for epoch in tqdm(range(config.max_epochs)):\n",
        "    total_loss = 0\n",
        "    momentum_val = cosine_schedule(epoch, config.max_epochs, 0.996, 1)\n",
        "    for (x0, x1), _, _ in dataloader:\n",
        "        update_momentum(model.backbone, model.backbone_momentum, m=momentum_val)\n",
        "        update_momentum(\n",
        "            model.projection_head, model.projection_head_momentum, m=momentum_val\n",
        "        )\n",
        "        x0 = x0.to(device)\n",
        "        x1 = x1.to(device)\n",
        "        p0 = model(x0)\n",
        "        z0 = model.forward_momentum(x0)\n",
        "        p1 = model(x1)\n",
        "        z1 = model.forward_momentum(x1)\n",
        "        loss = 0.5 * (criterion(p0, z1) + criterion(p1, z0))\n",
        "        total_loss += loss.detach()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_corr = compute_average_off_correlation_matrix(z0,z1)\n",
        "    entropy_z0 = entropy(z0)\n",
        "    entropy_z1 = entropy(z1)\n",
        "    log_dict_BYOL[\"avg_loss\"].append(avg_loss)\n",
        "    log_dict_BYOL[\"avg_corr\"].append(avg_corr)\n",
        "    log_dict_BYOL[\"entropy_z0\"].append(entropy_z0)\n",
        "    log_dict_BYOL[\"entropy_z1\"].append(entropy_z1)\n",
        "    # print(f\"epoch: {epoch:>02}, loss: {avg_loss:.5f}, avg cross-correlation: {avg_corr}, mutual information: {mut_info} \")\n",
        "    wandb.log({\"avg_loss\": avg_loss, \"avg_corr\": avg_corr, \"entropy_z0\": entropy_z0, \"entropy_z1\": entropy_z1})\n",
        "\n",
        "# time to train \n",
        "end = time.time()\n",
        "train_time = time.strftime(\"%H:%M:%S\", time.gmtime(end - start))\n",
        "print(\"Time for the training :\", train_time)\n",
        "# compute singular values\n",
        "singular_values_BYOL = compute_singular_values(z0, z1)\n",
        "\n",
        "# plot singular values\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(singular_values_BYOL.size),singular_values_BYOL, label=f'Singular Values')\n",
        "ax.set_yscale('log')\n",
        "ax.set_xlabel('Singular Value Index')\n",
        "ax.set_ylabel('Singular Value')\n",
        "wandb.log({\"Log Singular Values \": fig})\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'BYOL-Vanilla.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "Rm38TXmirv_g",
        "AztAO4TRsWNl",
        "Aow6wQ5X2Elj",
        "A30xLbF62Elk",
        "M0PiRWIG2Ell",
        "_jSl0hzw2Ell"
      ],
      "authorship_tag": "ABX9TyNN72exbaXw2ytpoorag+dE",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}